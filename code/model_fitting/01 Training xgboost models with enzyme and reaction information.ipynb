{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key text.latex.preview in file CCB_plot_style_0v4.mplstyle, line 55 ('text.latex.preview  : False')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key mathtext.fallback_to_cm in file CCB_plot_style_0v4.mplstyle, line 63 ('mathtext.fallback_to_cm : True ## When True, use symbols from the Computer Modern fonts')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from joblib import Parallel, delayed\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib as mpl\n",
    "plt.style.use('CCB_plot_style_0v4.mplstyle')\n",
    "c_styles      = mpl.rcParams['axes.prop_cycle'].by_key()['color']   # fetch the defined color styles\n",
    "high_contrast = ['#004488', '#DDAA33', '#BB5566', '#000000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3391, 869)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_pickle(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"train_df_kcat.pkl\"))\n",
    "data_test = pd.read_pickle(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"test_df_kcat.pkl\"))\n",
    "\n",
    "\n",
    "data_train.rename(columns = {\"geomean_kcat\" :\"log10_kcat\"}, inplace = True)\n",
    "data_test.rename(columns = {\"geomean_kcat\" :\"log10_kcat\"}, inplace = True)\n",
    "len(data_train), len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = list(np.load(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"CV_train_indices.npy\"), allow_pickle = True))\n",
    "test_indices = list(np.load(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"CV_test_indices.npy\"), allow_pickle = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training a model with only sequence information (ESM-1b):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ESM1b = np.array(list(data_train[\"ESM1b\"]))\n",
    "train_X = train_ESM1b\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_ESM1b = np.array(list(data_test[\"ESM1b\"]))\n",
    "test_X = test_ESM1b\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [25:13<00:00,  7.57s/trial, best loss: 0.03743723040132387]\n"
     ]
    }
   ],
   "source": [
    "'''def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, rand, hp, Trials\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n",
    "    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.051447544749765035,\n",
    "         'max_delta_step': 2.956459783615207,\n",
    "         'max_depth': 5.034202474908222,\n",
    "         'min_child_weight': 7.457989829577018,\n",
    "         'num_rounds': 297.50601395689256,\n",
    "         'reg_alpha': 1.0858835704466614, \n",
    "         'reg_lambda': 1.1385559144302175}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.10233607311660751, -0.15314606513040566, -0.0455418650437268, -0.031018786560747122, -0.04521997323856698]\n",
      "[1.8346361248595169, 1.6485991485910756, 1.6585845403859683, 1.6521416228683834, 1.5457048408922371]\n",
      "[-0.14527321350278588, -0.28902782085496215, -0.178978678895795, -0.13075785627107317, -0.16639229744573591]\n"
     ]
    }
   ],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\",  \"training_results\", \"MSE_CV_xgboost_ESM1b.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.077 1.6 -0.171\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_ESM1b.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training a model with only sequence information (ESM-1b_ts):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ESM1b = np.array(list(data_train[\"ESM1b_ts\"]))\n",
    "train_X = train_ESM1b\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_ESM1b = np.array(list(data_test[\"ESM1b_ts\"]))\n",
    "test_X = test_ESM1b\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, rand, hp, Trials\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n",
    "    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.2831145406836757,\n",
    "        'max_delta_step': 0.07686715986169101, \n",
    "        'max_depth': 4.96836783761305,\n",
    "        'min_child_weight': 6.905400087083855,\n",
    "        'num_rounds': 313.1498988074061,\n",
    "        'reg_alpha': 1.717314107718892,\n",
    "        'reg_lambda': 2.470354543039016}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02486240565545573, -0.1420797075589647, -0.10379250527068316, -0.10016291955822776, 0.020334890011688145]\n",
      "[1.7813953024416926, 1.7386608227826128, 1.7782258113705949, 1.8222451562799455, 1.568066049170657]\n",
      "[-0.11203758331226976, -0.35944639636185927, -0.26402379065961523, -0.24718002258067173, -0.18326611475387966]\n"
     ]
    }
   ],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "y_valid_pred_esm1b_ts = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    y_valid_pred_esm1b_ts.append(y_valid_pred)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b_ts.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\",  \"training_results\", \"MSE_CV_xgboost_ESM1b_ts.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b_ts.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1 1.689 -0.236\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b_ts.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_ESM1b_ts.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ESM1b = np.array(list(data_train[\"ESM1b_ts\"]))\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_ESM1b = np.array(list(data_test[\"ESM1b_ts\"]))\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))\n",
    "\n",
    "train_X = np.concatenate([train_ESM1b, test_ESM1b])\n",
    "train_Y = np.concatenate([train_Y, test_Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9669336684582881, 0.0) 0.12759116635142767 0.906604299121613\n"
     ]
    }
   ],
   "source": [
    "param = {'learning_rate': 0.2831145406836757,\n",
    "         'max_delta_step': 0.07686715986169101, \n",
    "         'max_depth': 4.96836783761305,\n",
    "          'min_child_weight': 6.905400087083855,\n",
    "           'num_rounds': 313.1498988074061,\n",
    "            'reg_alpha': 1.717314107718892,\n",
    "             'reg_lambda': 2.470354543039016}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"training_results\", \"saved_models\",\n",
    "                          \"xgboost_sequence_only_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a model with only reaction information (DRFP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"DRFP\"]))\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"DRFP\"]))\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, rand, hp, Trials\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n",
    "    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.08987247189322463,\n",
    "         'max_delta_step': 1.1939737318908727,\n",
    "         'max_depth': 11.268531225242574,\n",
    "         'min_child_weight': 2.8172720953826302,\n",
    "         'num_rounds': 109.03643430746544,\n",
    "         'reg_alpha': 1.9412226989868904,\n",
    "         'reg_lambda': 4.950543905603358}\n",
    "\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5881133955099471, 0.5462132054748297, 0.5819635238384331, 0.5726775753682218, 0.5511282628249307]\n",
      "[1.049007141620355, 0.9050367902176091, 0.9322238709756645, 0.9829545115130323, 0.9227964694050949]\n",
      "[0.3451563698153438, 0.29235824094365936, 0.3373433544833062, 0.32724681657693344, 0.30365573973170834]\n"
     ]
    }
   ],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "y_valid_pred_DRFP = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    y_valid_pred_DRFP.append(y_valid_pred)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_DRFP.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\",  \"training_results\", \"MSE_CV_xgboost_DRFP.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_DRFP.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.559 0.945 0.308\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_DRFP.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_DRFP.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_drfp = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DRFP = np.array(list(data_train[\"DRFP\"]))\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_DRFP = np.array(list(data_test[\"DRFP\"]))\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))\n",
    "\n",
    "train_X = np.concatenate([train_DRFP, test_DRFP])\n",
    "train_Y = np.concatenate([train_Y, test_Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8334966832772743, 3.3104605748123116e-229) 0.4721235730299549 0.6544093665317637\n"
     ]
    }
   ],
   "source": [
    "param = {'learning_rate': 0.08987247189322463,\n",
    "         'max_delta_step': 1.1939737318908727,\n",
    "         'max_depth': 11.268531225242574,\n",
    "         'min_child_weight': 2.8172720953826302,\n",
    "         'num_rounds': 109.03643430746544,\n",
    "         'reg_alpha': 1.9412226989868904,\n",
    "         'reg_lambda': 4.950543905603358}\n",
    "\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"training_results\", \"saved_models\",\n",
    "                          \"xgboost_reaction_only_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training a model with only reaction information (difference fingerprint):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, rand, hp, Trials\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n",
    "    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.14154883958006167,\n",
    "         'max_delta_step': 0.02234358170535966,\n",
    "         'max_depth': 10.869653004093198,\n",
    "         'min_child_weight': 1.7936882442746056,\n",
    "         'num_rounds': 361.6168542774665,\n",
    "         'reg_alpha': 4.825525325323308, \n",
    "         'reg_lambda': 2.74944090578774}\n",
    "\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5782422525621318, 0.527702418863558, 0.6225643830666265, 0.5591353626322173, 0.5493683223243451]\n",
      "[1.0982538548617198, 0.9290581544463321, 0.887808524875404, 1.0179675533983907, 0.9407644513087896]\n",
      "[0.3144140657888673, 0.27357610896683715, 0.3689153032100193, 0.303283209803877, 0.29009706077912356]\n"
     ]
    }
   ],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.519 1.001 0.267\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_diff_fp = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training a model with only reaction information (structural fingerprint):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = ();\n",
    "for ind in data_train.index:\n",
    "    train_X = train_X + (np.array(list(data_train[\"structural_fp\"][ind])).astype(int), )\n",
    "train_X = np.array(train_X)\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "\n",
    "test_X = ();\n",
    "for ind in data_test.index:\n",
    "    test_X = test_X + (np.array(list(data_test[\"structural_fp\"][ind])).astype(int), )\n",
    "test_X = np.array(test_X)\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, rand, hp, Trials\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n",
    "    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.01126910440903659,\n",
    "         'max_delta_step': 0.5777120839605732,\n",
    "         'max_depth': 5.486901609313889,\n",
    "         'min_child_weight': 6.14467742389769,\n",
    "         'num_rounds': 488.943459090126,\n",
    "         'reg_alpha': 4.629840853377147,\n",
    "         'reg_lambda': 2.1047561335691745}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5461680032633288, 0.5239731776116427, 0.5969058521176137, 0.5601464413573072, 0.5274004514654467]\n",
      "[1.1329108964865013, 0.9326878465396904, 0.9177482860266996, 1.0106627733905318, 0.9644893012025042]\n",
      "[0.292779377092681, 0.27073807881652767, 0.3476330958885909, 0.30828274329900063, 0.2721942364869312]\n"
     ]
    }
   ],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_str_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_str_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_str_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.524 0.992 0.274\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_str_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_str_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training a model with enzyme and reaction information (ESM1b_ts/DRFP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"DRFP\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"ESM1b_ts\"]))], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"DRFP\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"ESM1b_ts\"]))], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, rand, hp, Trials\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    \"max_depth\": hp.uniform(\"max_depth\", 6,14),\n",
    "    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.05221672412884108,\n",
    "         'max_delta_step': 1.0767235463496743,\n",
    "         'max_depth': 11.329014411591299,\n",
    "         'min_child_weight': 14.724796449973605,\n",
    "         'num_rounds': 298.9598325756988,\n",
    "         'reg_alpha': 2.8295816318634452,\n",
    "         'reg_lambda': 0.6528469146574993}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.431459139611596, 0.3200005154441318, 0.408405900279767, 0.34895902652111643, 0.3510372955800634]\n",
      "[1.3037907166297578, 1.1896381263532094, 1.1765466577947605, 1.2925965723906208, 1.1726797961960855]\n",
      "[0.18610749917289393, 0.06983050250293021, 0.16367035234554095, 0.11532176843159525, 0.11509322771881669]\n"
     ]
    }
   ],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b_ts_DRFP.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_ESM1b_ts_DRFP.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b_ts_DRFP.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33 1.237 0.095\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b_ts_DRFP.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_ESM1b_ts_DRFP.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training a model with enzyme and reaction information (ESM1b_ts/diff_fp):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"ESM1b_ts\"]))], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"ESM1b_ts\"]))], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [1:14:30<00:00, 22.35s/trial, best loss: -0.4463204621674697]\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "\n",
    "    R2 = []\n",
    "    skf = KFold(n_splits=5)\n",
    "\n",
    "    def train_and_evaluate(train_index, test_index):\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        mse = np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2)\n",
    "        r2 = r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred)\n",
    "        return mse, r2\n",
    "    \n",
    "    results = []\n",
    "    for train_index, test_index in skf.split(train_X, train_Y):\n",
    "        result = train_and_evaluate(train_index, test_index)\n",
    "        results.append(result)\n",
    "\n",
    "    MSE, R2 = zip(*results)\n",
    "    return -np.mean(R2)\n",
    "\n",
    "space = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n",
    "    \"gamma\": hp.uniform(\"gamma\", 0, 0.5),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space=space, algo=tpe.suggest, max_evals = 200, trials=trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.7706304470135901,\n",
       " 'gamma': 0.08473280513934948,\n",
       " 'learning_rate': 0.08292031202222186,\n",
       " 'max_delta_step': 2.0905240205907973,\n",
       " 'max_depth': 8.018890262009021,\n",
       " 'min_child_weight': 1.152288086135032,\n",
       " 'num_rounds': 188.30718240661182,\n",
       " 'reg_alpha': 2.097680263149934,\n",
       " 'reg_lambda': 0.09212019804805793}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''param = {'learning_rate': 0.5727401435817077, \n",
    "         'max_delta_step': 0.022572978136953803,\n",
    "         'max_depth': 9.734956573895278,\n",
    "         'min_child_weight': 2.026404280518698,\n",
    "         'num_rounds': 259.69265795096726,\n",
    "         'reg_alpha': 7.333074414515098,\n",
    "         'reg_lambda': 0.8545111451043885}''';\n",
    "\n",
    "'''param = {'learning_rate': 0.029815704092754386,\n",
    "        'max_delta_step': 1.506753151302061,\n",
    "        'max_depth': 7.916290092522673,\n",
    "        'min_child_weight': 9.004351370580997,\n",
    "        'num_rounds': 162.51704178700743,\n",
    "        'reg_alpha': 2.415815193301269,\n",
    "        'reg_lambda': 4.548495397904814}''';\n",
    "\n",
    "param = {'colsample_bytree': 0.7706304470135901,\n",
    "        'gamma': 0.08473280513934948,\n",
    "        'learning_rate': 0.08292031202222186,\n",
    "        'max_delta_step': 2.0905240205907973,\n",
    "        'max_depth': 8.018890262009021,\n",
    "        'min_child_weight': 1.152288086135032,\n",
    "        'num_rounds': 188.30718240661182,\n",
    "        'reg_alpha': 2.097680263149934,\n",
    "        'reg_lambda': 0.09212019804805793}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "param[\"tree_method\"] = \"gpu_hist\"\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.425 1.028 0.175\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "# 0.395 1.16 0.151 old old params\n",
    "# 0.368 1.098 0.119 old params\n",
    "# 0.424 1.024 0.179 new params\n",
    "# 0.425 1.028 0.175 new new params\n",
    "\n",
    "#np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b_ts_diff_fp.npy\"), bst.predict(dtest))\n",
    "#np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_ESM1b_ts_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on all the data\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98 0.071 0.943\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0], 3), np.round(MSE_dif_fp_test, 3), np.round(R2_dif_fp_test, 3))\n",
    "# 0.989 0.033 0.973 old old params\n",
    "# 0.982 0.053 0.957 old params\n",
    "# 0.904 0.348 0.721 new params\n",
    "# 0.98 0.071 0.943 new new params\n",
    "\n",
    "#pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"training_results\", \"saved_models\", \"xgboost_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model with enzyme and reaction information (ESM1b_ts/DRFP [mean]):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48885823200781836, 0.354974544097536, 0.455574304946437, 0.4236548592696329, 0.43460493901700165]\n",
      "[1.2485969063138214, 1.1331299043335048, 1.1332844703900258, 1.2103006866579678, 1.0815445450679488]\n",
      "[0.22056228377539155, 0.11401387500600313, 0.19442259638895099, 0.17164667305413162, 0.18386409013022098]\n"
     ]
    }
   ],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    y_valid_pred = np.mean([y_valid_pred_DRFP[i], y_valid_pred_esm1b_ts[i]], axis =0)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b_ts_DRFP_mean.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_ESM1b_ts_DRFP_mean.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b_ts_DRFP_mean.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.424 1.127 0.175\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = np.mean([y_test_pred_drfp, y_test_pred_esm1b_ts], axis =0)\n",
    "\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b_ts_DRFP_mean.npy\"), y_test_pred)\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_ESM1b_ts_DRFP_mean.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training a model with enzyme and reaction information (ESM2_3B/diff_fp):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"ESM2_3B\"]))], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"ESM2_3B\"]))], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [3:36:58<00:00, 65.09s/trial, best loss: -0.3579976419781222]  \n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(train_index, test_index):\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "\n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    r2 = r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred)\n",
    "    return r2\n",
    "\n",
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "\n",
    "    R2 = []\n",
    "    kf = KFold(n_splits=5)\n",
    "    \n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        result = train_and_evaluate(train_index, test_index)\n",
    "        R2.append(result)\n",
    "\n",
    "    return -np.mean(R2)\n",
    "\n",
    "space = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.001, 1),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 1, 20, 1),\n",
    "    \"gamma\": hp.uniform(\"gamma\", 0, 1),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.1, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 10),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 10),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 10),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.01, 20),\n",
    "    \"num_rounds\":  hp.quniform(\"num_rounds\", 10, 300, 10)\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space=space, algo=tpe.suggest, max_evals = 200, trials=trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.9965828569106562,\n",
       " 'gamma': 0.9950146409429289,\n",
       " 'learning_rate': 0.7886969584173175,\n",
       " 'max_delta_step': 8.387703310583698,\n",
       " 'max_depth': 3.0,\n",
       " 'min_child_weight': 18.34298401258303,\n",
       " 'num_rounds': 290.0,\n",
       " 'reg_alpha': 7.564932934795392,\n",
       " 'reg_lambda': 8.456788423168604}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''param = {'learning_rate': 0.5727401435817077, \n",
    "        'max_delta_step': 0.022572978136953803,\n",
    "        'max_depth': 9.734956573895278,\n",
    "        'min_child_weight': 2.026404280518698,\n",
    "        'num_rounds': 259.69265795096726,\n",
    "        'reg_alpha': 7.333074414515098,\n",
    "        'reg_lambda': 0.8545111451043885}''';\n",
    "\n",
    "'''param = {'colsample_bytree': 0.5033789302958038,\n",
    "        'gamma': 0.0037752141604097122,\n",
    "        'learning_rate': 0.061921864701214496,\n",
    "        'max_delta_step': 0.9351742448381885,\n",
    "        'max_depth': 10.7625991614889,\n",
    "        'min_child_weight': 13.939420814219764,\n",
    "        'num_rounds': 144.57893540285642,\n",
    "        'reg_alpha': 2.5404285747350235,\n",
    "        'reg_lambda': 2.0705777927656057}''';\n",
    "\n",
    "'''param = {'colsample_bytree': 0.3404528115480022,\n",
    "        'gamma': 0.6219875598661728,\n",
    "        'learning_rate': 0.984051033355358,\n",
    "        'max_delta_step': 6.438638552418567,\n",
    "        'max_depth': 10.0,\n",
    "        'min_child_weight': 3.6623047466798453,\n",
    "        'num_rounds': 160.0,\n",
    "        'reg_alpha': 6.137824021103583,\n",
    "        'reg_lambda': 4.284160185624403}''';\n",
    "\n",
    "param = {'colsample_bytree': 0.9965828569106562,\n",
    "        'gamma': 0.9950146409429289,\n",
    "        'learning_rate': 0.7886969584173175,\n",
    "        'max_delta_step': 8.387703310583698,\n",
    "        'max_depth': 3.0,\n",
    "        'min_child_weight': 18.34298401258303,\n",
    "        'num_rounds': 290.0,\n",
    "        'reg_alpha': 7.564932934795392,\n",
    "        'reg_lambda': 8.456788423168604}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "param[\"tree_method\"] = \"gpu_hist\"\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.41, 1.346, 0.046)\n",
      "(0.515, 1.16, 0.154)\n",
      "(0.476, 1.302, 0.157)\n",
      "(0.444, 1.286, 0.024)\n",
      "(0.485, 1.359, 0.136)\n"
     ]
    }
   ],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "\n",
    "    MSE.append(np.round(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2),3))\n",
    "    R2.append(np.round(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred),3))\n",
    "    Pearson.append(np.round(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0],3))\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"({Pearson[i]}, {MSE[i]}, {R2[i]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.483 1.079 0.135\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "# 0.607 0.789 0.367 old params\n",
    "# 0.586 0.821 0.342\n",
    "# 0.966 0.086 0.931\n",
    "# 0.483 1.079 0.135\n",
    "\n",
    "#np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_ESM2_3B_diff_fp.npy\"), bst.predict(dtest))\n",
    "#np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_ESM2_3B_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_ESM2_3B_drfp = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948 0.131 0.895\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "# 0.986 0.037 0.971 old params\n",
    "# 0.987 0.036 0.971\n",
    "# 0.983 0.041 0.967\n",
    "# 0.948 0.131 0.895\n",
    "\n",
    "#pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"training_results\", \"saved_models\", \"xgboost_train_and_test_ESM2_3B.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training a model with enzyme rep., reaction information, and additional features (ESM1b_ts/diff_fp/flux/KM):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset with additonal features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3421, 850)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_pickle(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"train_df_kcat_with_KM_and_flux.pkl\"))\n",
    "data_test = pd.read_pickle(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"test_df_kcat_with_KM_and_flux.pkl\"))\n",
    "\n",
    "data_train.rename(columns = {\"geomean_kcat\" :\"log10_kcat\"}, inplace = True)\n",
    "data_test.rename(columns = {\"geomean_kcat\" :\"log10_kcat\"}, inplace = True)\n",
    "len(data_train), len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = ();\n",
    "train_X = np.array(list(data_train[\"DRFP\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"ESM1b_ts\"])),\n",
    "                          np.reshape(np.array(list(data_train[\"KM\"])), (-1,1)),\n",
    "                          np.reshape(np.array(list(data_train[\"flux\"])), (-1,1))], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_X = ();\n",
    "test_X = np.array(list(data_test[\"DRFP\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"ESM1b_ts\"])),\n",
    "                          np.reshape(np.array(list(data_test[\"KM\"])), (-1,1)),\n",
    "                          np.reshape(np.array(list(data_test[\"flux\"])), (-1,1))], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, rand, hp, Trials\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n",
    "    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.15055870206296312,\n",
    "         'max_delta_step': 0.13821534396910307,\n",
    "         'max_depth': 5.338955142881738,\n",
    "         'min_child_weight': 14.84613730467497,\n",
    "         'num_rounds': 294.13028718637383,\n",
    "         'reg_alpha': 2.6752278199969153,\n",
    "         'reg_lambda': 0.6063171152564584}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b_diff_fp_flux_KM.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_ESM1b_diff_fp_flux_KM.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b_diff_fp_flux_KM.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson[0], MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b_diff_fp_flux_KM.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_ESM1b_diff_fp_flux_KM.npy\"), test_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
